{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXCU8ZgC9Cb5"
      },
      "source": [
        "# Dot Product Program\n",
        "\n",
        "Group 1 - CEPARCO Project\n",
        "\n",
        "``Cai, Edison``\n",
        "``Susada, Stephanie Joy``\n",
        "\n",
        "Shared Memory in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exqGVXvCQm7f"
      },
      "source": [
        "### C++ Implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J25UE3eU6KPi",
        "outputId": "5261f80d-e230-4334-842a-36b96344551c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting c_dotp.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile c_dotp.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "//kernel function\n",
        "void f_dotp(int n, float* h_dotp, float* h_x, float* h_y){\n",
        "    for(int i = 0; i < n; i++)\n",
        "        *h_dotp += h_x[i] * h_y[i];\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const unsigned int ARRAY_SIZE = 1<<24;\n",
        "    const unsigned ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    //declare array\n",
        "    float dotp = 0, *X, *Y;\n",
        "    X = (float*)malloc(ARRAY_BYTES);\n",
        "    Y = (float*)malloc(ARRAY_BYTES);\n",
        "\n",
        "    clock_t start, end;\n",
        "\n",
        "    //initialize array\n",
        "    srand(time(0));\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        X[i] = (rand() % 10) + 1;\n",
        "        Y[i] = (rand() % 10) + 1;\n",
        "    }\n",
        "    \n",
        "    //print statements for verification\n",
        "    /*\n",
        "    printf(\"X: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", X[i]);\n",
        "    }\n",
        "    printf(\"\\nY: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", Y[i]);\n",
        "    }\n",
        "    */\n",
        "\n",
        "    f_dotp(ARRAY_SIZE, &dotp, X, Y);\n",
        "    dotp = 0;\n",
        "\n",
        "    start = clock();\n",
        "    f_dotp(ARRAY_SIZE, &dotp, X, Y);\n",
        "    end = clock();\n",
        "\n",
        "    double time_taken = ((double)(end-start))*1e6/ CLOCKS_PER_SEC;\n",
        "\n",
        "    printf(\"\\ndotp: %f\", dotp);\n",
        "\n",
        "    printf(\"\\nC function took %f us for array size %d \\n\", time_taken, ARRAY_SIZE);\n",
        "\n",
        "    //check for errors\n",
        "    float temp = 0;\n",
        "    for(int i = 0; i<ARRAY_SIZE; i++){\n",
        "        temp += X[i] * Y[i];\n",
        "    }\n",
        "    if (temp != dotp)\n",
        "        printf(\"There is an error with the result.\");\n",
        "    else\n",
        "        printf(\"No errors were found.\");\n",
        "\n",
        "    free(X);\n",
        "    free(Y);\n",
        "    return 0;\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGM8nQfvBeVO",
        "outputId": "41651f07-70da-4574-ea20-47e1c6fa4f9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "%%shell\n",
        "g++ c_dotp.c -o c_dotp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGil20T3DyDV",
        "outputId": "35967b48-8903-4df2-8c2b-9f41f46a47c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "dotp: 498392928.000000\n",
            "C function took 51344.000000 us for array size 16777216 \n",
            "No errors were found."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "%%shell\n",
        "./c_dotp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3G7XB_ZQN3o"
      },
      "source": [
        "### CUDA Implementation w/o shared memory using grid-stride loop w/ prefetching + mem advise: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Adqg-RZRQNc4",
        "outputId": "d8d01084-5270-48b9-b655-3a86bac56d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_dotp_notshared.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotp_notshared.cu\n",
        "#include <cuda_profiler_api.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "__global__\n",
        "void f_dotp(int n, float* h_dotp, float* h_x, float* h_y){\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    for(int i = index; i < n; i += stride){\n",
        "        atomicAdd(h_dotp, (h_x[i]*h_y[i]));\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const unsigned int ARRAY_SIZE = 1<<20;\n",
        "    const unsigned ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    int numThread = 1024;\n",
        "    int numBlock = (ARRAY_SIZE+numThread-1) / numThread;\n",
        "\n",
        "    float *dotp = 0, *X, *Y;\n",
        "    cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&dotp, sizeof(float));\n",
        "\n",
        "    srand(time(0));\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        X[i] = (rand() % 10) + 1;\n",
        "        Y[i] = (rand() % 10) + 1;\n",
        "        //X[i] = (float)rand()/RAND_MAX;\n",
        "        //Y[i] = (float)rand()/RAND_MAX;\n",
        "    }\n",
        "\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "    printf(\"Device # = %d\", device);\n",
        "    cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(dotp, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(dotp, sizeof(float), cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, device);\n",
        "    cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, device);\n",
        "\n",
        "    /*\n",
        "    printf(\"X: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", X[i]);\n",
        "    }\n",
        "    printf(\"\\nY: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", Y[i]);\n",
        "    }\n",
        "    */\n",
        "\n",
        "    f_dotp<<<numBlock, numThread>>>(ARRAY_SIZE, dotp, X, Y);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(dotp, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    printf(\"\\ndotp = %f\\n\", *dotp);\n",
        "\n",
        "    float temp = 0;\n",
        "    for(int i = 0; i<ARRAY_SIZE; i++){\n",
        "        temp += X[i] * Y[i];\n",
        "    }\n",
        "    if (temp != *dotp)\n",
        "        printf(\"There is an error with the result.\\n\");\n",
        "    else\n",
        "        printf(\"No errors were found.\\n\");\n",
        "\n",
        "    cudaFree(X);\n",
        "    cudaFree(Y);\n",
        "    cudaFree(dotp);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNbqlRuRHZgw",
        "outputId": "a389e37c-0180-4d52-8a03-643293b34e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotp_notshared.cu -o cuda_dotp_notshared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiOdn3pnICEg",
        "outputId": "d5868b76-ebf6-477c-c85c-82511dd70124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==5257== NVPROF is profiling process 5257, command: ./cuda_dotp_notshared\n",
            "Device # = 0\n",
            "dotp = 261906.781250\n",
            "There is an error with the result.\n",
            "==5257== Profiling application: ./cuda_dotp_notshared\n",
            "==5257== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  906.09ms         1  906.09ms  906.09ms  906.09ms  f_dotp(int, float*, float*, float*)\n",
            "      API calls:   85.30%  906.59ms         1  906.59ms  906.59ms  906.59ms  cudaDeviceSynchronize\n",
            "                   14.51%  154.22ms         3  51.407ms  14.015us  154.17ms  cudaMallocManaged\n",
            "                    0.07%  732.10us         3  244.03us  64.800us  377.34us  cudaFree\n",
            "                    0.06%  630.80us         6  105.13us  1.9530us  442.08us  cudaMemPrefetchAsync\n",
            "                    0.04%  417.95us         5  83.590us  5.0520us  305.49us  cudaMemAdvise\n",
            "                    0.01%  125.32us       101  1.2400us     133ns  53.897us  cuDeviceGetAttribute\n",
            "                    0.00%  41.033us         1  41.033us  41.033us  41.033us  cudaLaunchKernel\n",
            "                    0.00%  24.305us         1  24.305us  24.305us  24.305us  cuDeviceGetName\n",
            "                    0.00%  7.0110us         1  7.0110us  7.0110us  7.0110us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.1440us         1  5.1440us  5.1440us  5.1440us  cudaGetDevice\n",
            "                    0.00%  1.9100us         3     636ns     194ns  1.3830us  cuDeviceGetCount\n",
            "                    0.00%  1.0640us         2     532ns     290ns     774ns  cuDeviceGet\n",
            "                    0.00%     507ns         1     507ns     507ns     507ns  cuModuleGetLoadingMode\n",
            "                    0.00%     430ns         1     430ns     430ns     430ns  cuDeviceTotalMem\n",
            "                    0.00%     214ns         1     214ns     214ns     214ns  cuDeviceGetUuid\n",
            "\n",
            "==5257== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       4  2.0000MB  2.0000MB  2.0000MB  8.000000MB  704.0800us  Host To Device\n",
            "      24  170.67KB  4.0000KB  0.9961MB  4.000000MB  351.7690us  Device To Host\n",
            "       1         -         -         -           -  124.8930us  Gpu page fault groups\n",
            "       1  4.0000KB  4.0000KB  4.0000KB  4.000000KB           -  Remote mapping from device\n",
            "Total CPU Page faults: 37\n",
            "Total remote mappings to CPU: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotp_notshared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDqTDbjhY7UK"
      },
      "source": [
        "### CUDA Implementation w/ Shared Memory using grid-stride loop w/ prefetching + mem advise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I72CDg8vZFGq",
        "outputId": "8f65ef47-e166-440b-c3c1-c5eee8379d12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cuda_dotp_shared.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_dotp_shared.cu\n",
        "#include <cuda_profiler_api.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "__global__\n",
        "void f_dotp(int n, float* h_dotp, float* h_x, float* h_y){\n",
        "    __shared__ int t_dotp;\n",
        "    t_dotp = 0;\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "    for(int i = index; i < n; i += stride){\n",
        "        atomicAdd(&t_dotp, (h_x[i]*h_y[i]));\n",
        "        __syncthreads();\n",
        "        if(index % blockDim.x == 0)\n",
        "            atomicAdd(h_dotp, t_dotp);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    const unsigned int ARRAY_SIZE = 256;\n",
        "    const unsigned ARRAY_BYTES = ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    int numThread = 1024;\n",
        "    int numBlock = (ARRAY_SIZE+numThread-1) / numThread;\n",
        "\n",
        "    float *dotp = 0, *X, *Y;\n",
        "    cudaMallocManaged(&X, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&Y, ARRAY_BYTES);\n",
        "    cudaMallocManaged(&dotp, sizeof(float));\n",
        "\n",
        "    srand(time(0));\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        X[i] = (rand() % 10) + 1;\n",
        "        Y[i] = (rand() % 10) + 1;\n",
        "    }\n",
        "\n",
        "    int device = -1;\n",
        "    cudaGetDevice(&device);\n",
        "    printf(\"Device # = %d\", device);\n",
        "    cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(dotp, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(dotp, sizeof(float), cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId);\n",
        "    cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, device);\n",
        "    cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, device);\n",
        "\n",
        "    /*\n",
        "    printf(\"X: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", X[i]);\n",
        "    }\n",
        "    printf(\"\\nY: \");\n",
        "    for(int i = 0; i < ARRAY_SIZE; i++){\n",
        "        printf(\"%f, \", Y[i]);\n",
        "    }\n",
        "    */\n",
        "\n",
        "    f_dotp<<<numBlock, numThread>>>(ARRAY_SIZE, dotp, X, Y);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL);\n",
        "    cudaMemPrefetchAsync(dotp, ARRAY_BYTES, device, NULL);\n",
        "\n",
        "    printf(\"\\ndotp = %f\\n\", *dotp);\n",
        "\n",
        "    float temp = 0;\n",
        "    for(int i = 0; i<ARRAY_SIZE; i++){\n",
        "        temp += X[i] * Y[i];\n",
        "    }\n",
        "    if (temp != *dotp)\n",
        "        printf(\"There is an error with the result.\\n\");\n",
        "    else\n",
        "        printf(\"No errors were found.\\n\");    \n",
        "\n",
        "    cudaFree(X);\n",
        "    cudaFree(Y);\n",
        "    cudaFree(dotp);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gkv_6keTaYYz",
        "outputId": "0fef6bf8-72e9-4abd-cbf2-29782536f9b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc cuda_dotp_shared.cu -o cuda_dotp_shared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFF8OUhNafez",
        "outputId": "e8a365a0-3bf5-48fa-95a0-110cf8efc0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2463== NVPROF is profiling process 2463, command: ./cuda_dotp_shared\n",
            "Device # = 0\n",
            "\n",
            "dotp = 7743.000000\n",
            "No errors were found.\n",
            "==2463== Profiling application: ./cuda_dotp_shared\n",
            "==2463== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  79.902us         1  79.902us  79.902us  79.902us  f_dotp(int, float*, float*, float*)\n",
            "      API calls:   99.67%  245.60ms         3  81.866ms  4.8230us  245.57ms  cudaMallocManaged\n",
            "                    0.14%  344.53us         6  57.422us  1.3670us  210.77us  cudaMemPrefetchAsync\n",
            "                    0.06%  148.72us         3  49.572us  6.7950us  111.02us  cudaFree\n",
            "                    0.05%  116.22us       101  1.1500us     128ns  48.812us  cuDeviceGetAttribute\n",
            "                    0.03%  83.754us         1  83.754us  83.754us  83.754us  cudaDeviceSynchronize\n",
            "                    0.01%  33.855us         1  33.855us  33.855us  33.855us  cudaLaunchKernel\n",
            "                    0.01%  30.582us         5  6.1160us     951ns  21.047us  cudaMemAdvise\n",
            "                    0.01%  28.656us         1  28.656us  28.656us  28.656us  cuDeviceGetName\n",
            "                    0.00%  6.8540us         1  6.8540us  6.8540us  6.8540us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.2780us         1  3.2780us  3.2780us  3.2780us  cudaGetDevice\n",
            "                    0.00%  2.4420us         3     814ns     186ns  2.0170us  cuDeviceGetCount\n",
            "                    0.00%  1.0480us         2     524ns     203ns     845ns  cuDeviceGet\n",
            "                    0.00%     667ns         1     667ns     667ns     667ns  cuModuleGetLoadingMode\n",
            "                    0.00%     467ns         1     467ns     467ns     467ns  cuDeviceTotalMem\n",
            "                    0.00%     262ns         1     262ns     262ns     262ns  cuDeviceGetUuid\n",
            "\n",
            "==2463== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       1  4.0000KB  4.0000KB  4.0000KB  4.000000KB  3.201000us  Host To Device\n",
            "       1  4.0000KB  4.0000KB  4.0000KB  4.000000KB  1.760000us  Device To Host\n",
            "       1         -         -         -           -  72.12700us  Gpu page fault groups\n",
            "Total CPU Page faults: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%shell\n",
        "nvprof ./cuda_dotp_shared"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "exqGVXvCQm7f",
        "n3G7XB_ZQN3o",
        "pDqTDbjhY7UK"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}